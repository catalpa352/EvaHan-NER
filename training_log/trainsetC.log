BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
/home/jinaai/miniconda3/envs/zihong-py310/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1, Batch 1, Batch Loss: 32.01274108886719
Epoch 1, Batch 2, Batch Loss: 20.918315887451172
Epoch 1, Batch 3, Batch Loss: 12.607458114624023
Epoch 1, Batch 4, Batch Loss: 8.913467407226562
Epoch 1, Batch 5, Batch Loss: 6.086230754852295
Epoch 1, Batch 6, Batch Loss: 3.4554293155670166
Epoch 1, Batch 7, Batch Loss: 3.016294240951538
Epoch 1, Batch 8, Batch Loss: 2.4991135597229004
Epoch 1, Batch 9, Batch Loss: 3.3357348442077637
Epoch 1, Batch 10, Batch Loss: 2.8614542484283447
Epoch 1, Batch 11, Batch Loss: 2.595951557159424
Epoch 1, Batch 12, Batch Loss: 2.7438371181488037
Epoch 1, Batch 13, Batch Loss: 2.043074131011963
Epoch 1, Batch 14, Batch Loss: 2.188743829727173
Epoch 1, Batch 15, Batch Loss: 2.5239522457122803
Epoch 1, Batch 16, Batch Loss: 2.0319254398345947
Epoch 1, Batch 17, Batch Loss: 1.6328507661819458
Epoch 1, Batch 18, Batch Loss: 1.9075795412063599
Epoch 1, Batch 19, Batch Loss: 1.9083526134490967
Epoch 1, Batch 20, Batch Loss: 1.7154802083969116
Epoch 1, Batch 21, Batch Loss: 1.5648751258850098
Epoch 1, Batch 22, Batch Loss: 1.8124181032180786
Epoch 1, Batch 23, Batch Loss: 1.4180729389190674
Epoch 1, Batch 24, Batch Loss: 1.5745577812194824
Epoch 1, Batch 25, Batch Loss: 1.5919153690338135
Epoch 1, Batch 26, Batch Loss: 1.1135668754577637
Epoch 1, Batch 27, Batch Loss: 1.4609352350234985
Epoch 1, Batch 28, Batch Loss: 1.73794424533844
Epoch 1, Batch 29, Batch Loss: 1.0309081077575684
Epoch 1, Batch 30, Batch Loss: 1.3828561305999756
Epoch 1, Batch 31, Batch Loss: 0.9241625070571899
Epoch 1, Batch 32, Batch Loss: 1.350354552268982
Epoch 1, Batch 33, Batch Loss: 1.0253385305404663
Epoch 1, Batch 34, Batch Loss: 1.1568763256072998
Epoch 1, Batch 35, Batch Loss: 1.3255131244659424
Epoch 1, Batch 36, Batch Loss: 1.0983189344406128
Epoch 1, Batch 37, Batch Loss: 0.9071590900421143
Epoch 1, Batch 38, Batch Loss: 1.1447101831436157
Epoch 1, Batch 39, Batch Loss: 1.0187593698501587
Epoch 1, Batch 40, Batch Loss: 0.9431765675544739
Epoch 1, Batch 41, Batch Loss: 1.0354279279708862
Epoch 1, Batch 42, Batch Loss: 0.7652048468589783
Epoch 1, Batch 43, Batch Loss: 0.8591969609260559
Epoch 1, Batch 44, Batch Loss: 0.8571110963821411
Epoch 1, Batch 45, Batch Loss: 0.821967363357544
Epoch 1, Batch 46, Batch Loss: 0.7149359583854675
Epoch 1, Batch 47, Batch Loss: 0.88187575340271
Epoch 1, Batch 48, Batch Loss: 0.8217406868934631
Epoch 1, Batch 49, Batch Loss: 0.7547428607940674
Epoch 1, Batch 50, Batch Loss: 0.8221144080162048
Epoch 1, Batch 51, Batch Loss: 0.7840157747268677
Epoch 1, Batch 52, Batch Loss: 0.8424290418624878
Epoch 1, Batch 53, Batch Loss: 0.6649676561355591
Epoch 1, Batch 54, Batch Loss: 0.722261369228363
Epoch 1, Batch 55, Batch Loss: 0.6738508939743042
Epoch 1, Batch 56, Batch Loss: 0.7355666756629944
Epoch 1, Batch 57, Batch Loss: 0.6827479004859924
Epoch 1, Batch 58, Batch Loss: 0.7605997920036316
Epoch 1, Batch 59, Batch Loss: 0.8606970906257629
Epoch 1, Batch 60, Batch Loss: 0.7439796924591064
Epoch 1, Batch 61, Batch Loss: 0.7229558825492859
Epoch 1, Batch 62, Batch Loss: 0.7256937026977539
Epoch 1, Batch 63, Batch Loss: 0.6782833337783813
Epoch 1, Batch 64, Batch Loss: 0.7695402503013611
Epoch 1, Batch 65, Batch Loss: 0.7700056433677673
Epoch 1, Batch 66, Batch Loss: 0.657031774520874
Epoch 1, Batch 67, Batch Loss: 0.6401410698890686
Epoch 1, Batch 68, Batch Loss: 0.5497840642929077
Epoch 1, Batch 69, Batch Loss: 0.6551187634468079
Epoch 1, Batch 70, Batch Loss: 0.6889653205871582
Epoch 1, Batch 71, Batch Loss: 0.5586628913879395
Epoch 1, Batch 72, Batch Loss: 0.5851674675941467
Epoch 1, Batch 73, Batch Loss: 0.5865371823310852
Epoch 1, Batch 74, Batch Loss: 0.5517088174819946
Epoch 1, Batch 75, Batch Loss: 0.503978431224823
Epoch 1, Batch 76, Batch Loss: 0.5600463151931763
Epoch 1, Average Loss: 2.218282377249316
Epoch 2, Batch 1, Batch Loss: 0.6266969442367554
Epoch 2, Batch 2, Batch Loss: 0.47793665528297424
Epoch 2, Batch 3, Batch Loss: 0.5398316383361816
Epoch 2, Batch 4, Batch Loss: 0.5527778267860413
Epoch 2, Batch 5, Batch Loss: 0.5905377268791199
Epoch 2, Batch 6, Batch Loss: 0.5125213861465454
Epoch 2, Batch 7, Batch Loss: 0.4380580186843872
Epoch 2, Batch 8, Batch Loss: 0.4757261872291565
Epoch 2, Batch 9, Batch Loss: 0.4349886476993561
Epoch 2, Batch 10, Batch Loss: 0.574557900428772
Epoch 2, Batch 11, Batch Loss: 0.49938222765922546
Epoch 2, Batch 12, Batch Loss: 0.4222826063632965
Epoch 2, Batch 13, Batch Loss: 0.456255704164505
Epoch 2, Batch 14, Batch Loss: 0.4314882159233093
Epoch 2, Batch 15, Batch Loss: 0.5085653066635132
Epoch 2, Batch 16, Batch Loss: 0.4298040568828583
Epoch 2, Batch 17, Batch Loss: 0.42746686935424805
Epoch 2, Batch 18, Batch Loss: 0.39113369584083557
Epoch 2, Batch 19, Batch Loss: 0.42085668444633484
Epoch 2, Batch 20, Batch Loss: 0.37004074454307556
Epoch 2, Batch 21, Batch Loss: 0.3377763628959656
Epoch 2, Batch 22, Batch Loss: 0.5106360912322998
Epoch 2, Batch 23, Batch Loss: 0.37977135181427
Epoch 2, Batch 24, Batch Loss: 0.3752208948135376
Epoch 2, Batch 25, Batch Loss: 0.32543298602104187
Epoch 2, Batch 26, Batch Loss: 0.3497689962387085
Epoch 2, Batch 27, Batch Loss: 0.41133853793144226
Epoch 2, Batch 28, Batch Loss: 0.326127827167511
Epoch 2, Batch 29, Batch Loss: 0.3337782919406891
Epoch 2, Batch 30, Batch Loss: 0.3512209951877594
Epoch 2, Batch 31, Batch Loss: 0.32960718870162964
Epoch 2, Batch 32, Batch Loss: 0.38431140780448914
Epoch 2, Batch 33, Batch Loss: 0.31336379051208496
Epoch 2, Batch 34, Batch Loss: 0.3401995301246643
Epoch 2, Batch 35, Batch Loss: 0.36291515827178955
Epoch 2, Batch 36, Batch Loss: 0.3797052800655365
Epoch 2, Batch 37, Batch Loss: 0.3183808922767639
Epoch 2, Batch 38, Batch Loss: 0.29000625014305115
Epoch 2, Batch 39, Batch Loss: 0.26948872208595276
Epoch 2, Batch 40, Batch Loss: 0.3229142427444458
Epoch 2, Batch 41, Batch Loss: 0.33164146542549133
Epoch 2, Batch 42, Batch Loss: 0.3595655858516693
Epoch 2, Batch 43, Batch Loss: 0.33476123213768005
Epoch 2, Batch 44, Batch Loss: 0.31628158688545227
Epoch 2, Batch 45, Batch Loss: 0.3203006386756897
Epoch 2, Batch 46, Batch Loss: 0.25590524077415466
Epoch 2, Batch 47, Batch Loss: 0.2835399806499481
Epoch 2, Batch 48, Batch Loss: 0.2815897762775421
Epoch 2, Batch 49, Batch Loss: 0.3052802085876465
Epoch 2, Batch 50, Batch Loss: 0.3532312512397766
Epoch 2, Batch 51, Batch Loss: 0.25746217370033264
Epoch 2, Batch 52, Batch Loss: 0.37045353651046753
Epoch 2, Batch 53, Batch Loss: 0.30926352739334106
Epoch 2, Batch 54, Batch Loss: 0.30164074897766113
Epoch 2, Batch 55, Batch Loss: 0.21078552305698395
Epoch 2, Batch 56, Batch Loss: 0.429979145526886
Epoch 2, Batch 57, Batch Loss: 0.27219897508621216
Epoch 2, Batch 58, Batch Loss: 0.30467721819877625
Epoch 2, Batch 59, Batch Loss: 0.2719149589538574
Epoch 2, Batch 60, Batch Loss: 0.3079349398612976
Epoch 2, Batch 61, Batch Loss: 0.1897173523902893
Epoch 2, Batch 62, Batch Loss: 0.23279817402362823
Epoch 2, Batch 63, Batch Loss: 0.36022430658340454
Epoch 2, Batch 64, Batch Loss: 0.3333757817745209
Epoch 2, Batch 65, Batch Loss: 0.2695009708404541
Epoch 2, Batch 66, Batch Loss: 0.2486429661512375
Epoch 2, Batch 67, Batch Loss: 0.33821406960487366
Epoch 2, Batch 68, Batch Loss: 0.23835836350917816
Epoch 2, Batch 69, Batch Loss: 0.19955487549304962
Epoch 2, Batch 70, Batch Loss: 0.30568355321884155
Epoch 2, Batch 71, Batch Loss: 0.3472972512245178
Epoch 2, Batch 72, Batch Loss: 0.2989445626735687
Epoch 2, Batch 73, Batch Loss: 0.2698633670806885
Epoch 2, Batch 74, Batch Loss: 0.41287094354629517
Epoch 2, Batch 75, Batch Loss: 0.21142719686031342
Epoch 2, Batch 76, Batch Loss: 0.20740151405334473
Epoch 2, Average Loss: 0.35835732637267365
Epoch 3, Batch 1, Batch Loss: 0.27253881096839905
Epoch 3, Batch 2, Batch Loss: 0.16118724644184113
Epoch 3, Batch 3, Batch Loss: 0.3519980311393738
Epoch 3, Batch 4, Batch Loss: 0.2488694190979004
Epoch 3, Batch 5, Batch Loss: 0.2501295208930969
Epoch 3, Batch 6, Batch Loss: 0.21934570372104645
Epoch 3, Batch 7, Batch Loss: 0.21277597546577454
Epoch 3, Batch 8, Batch Loss: 0.26502951979637146
Epoch 3, Batch 9, Batch Loss: 0.22230643033981323
Epoch 3, Batch 10, Batch Loss: 0.23828980326652527
Epoch 3, Batch 11, Batch Loss: 0.226775124669075
Epoch 3, Batch 12, Batch Loss: 0.17271989583969116
Epoch 3, Batch 13, Batch Loss: 0.2034696787595749
Epoch 3, Batch 14, Batch Loss: 0.1976538598537445
Epoch 3, Batch 15, Batch Loss: 0.2578434944152832
Epoch 3, Batch 16, Batch Loss: 0.2359277307987213
Epoch 3, Batch 17, Batch Loss: 0.15967199206352234
Epoch 3, Batch 18, Batch Loss: 0.21235431730747223
Epoch 3, Batch 19, Batch Loss: 0.19307465851306915
Epoch 3, Batch 20, Batch Loss: 0.2153329849243164
Epoch 3, Batch 21, Batch Loss: 0.19564209878444672
Epoch 3, Batch 22, Batch Loss: 0.23347406089305878
Epoch 3, Batch 23, Batch Loss: 0.23836730420589447
Epoch 3, Batch 24, Batch Loss: 0.24631786346435547
Epoch 3, Batch 25, Batch Loss: 0.18291065096855164
Epoch 3, Batch 26, Batch Loss: 0.21849137544631958
Epoch 3, Batch 27, Batch Loss: 0.1284647136926651
Epoch 3, Batch 28, Batch Loss: 0.2142486572265625
Epoch 3, Batch 29, Batch Loss: 0.2472849041223526
Epoch 3, Batch 30, Batch Loss: 0.2609873116016388
Epoch 3, Batch 31, Batch Loss: 0.17941023409366608
Epoch 3, Batch 32, Batch Loss: 0.24674257636070251
Epoch 3, Batch 33, Batch Loss: 0.1842147707939148
Epoch 3, Batch 34, Batch Loss: 0.2047821283340454
Epoch 3, Batch 35, Batch Loss: 0.22114340960979462
Epoch 3, Batch 36, Batch Loss: 0.25480377674102783
Epoch 3, Batch 37, Batch Loss: 0.23178735375404358
Epoch 3, Batch 38, Batch Loss: 0.178297221660614
Epoch 3, Batch 39, Batch Loss: 0.21181859076023102
Epoch 3, Batch 40, Batch Loss: 0.20440928637981415
Epoch 3, Batch 41, Batch Loss: 0.25202393531799316
Epoch 3, Batch 42, Batch Loss: 0.14898470044136047
Epoch 3, Batch 43, Batch Loss: 0.19284264743328094
Epoch 3, Batch 44, Batch Loss: 0.25265464186668396
Epoch 3, Batch 45, Batch Loss: 0.27671197056770325
Epoch 3, Batch 46, Batch Loss: 0.1527860313653946
Epoch 3, Batch 47, Batch Loss: 0.22471189498901367
Epoch 3, Batch 48, Batch Loss: 0.2195272296667099
Epoch 3, Batch 49, Batch Loss: 0.17547553777694702
Epoch 3, Batch 50, Batch Loss: 0.21569791436195374
Epoch 3, Batch 51, Batch Loss: 0.2217571884393692
Epoch 3, Batch 52, Batch Loss: 0.17290431261062622
Epoch 3, Batch 53, Batch Loss: 0.21777606010437012
Epoch 3, Batch 54, Batch Loss: 0.18098537623882294
Epoch 3, Batch 55, Batch Loss: 0.18945352733135223
Epoch 3, Batch 56, Batch Loss: 0.18256908655166626
Epoch 3, Batch 57, Batch Loss: 0.23087480664253235
Epoch 3, Batch 58, Batch Loss: 0.20292730629444122
Epoch 3, Batch 59, Batch Loss: 0.18936839699745178
Epoch 3, Batch 60, Batch Loss: 0.2526899576187134
Epoch 3, Batch 61, Batch Loss: 0.20152322947978973
Epoch 3, Batch 62, Batch Loss: 0.18395443260669708
Epoch 3, Batch 63, Batch Loss: 0.2193821519613266
Epoch 3, Batch 64, Batch Loss: 0.1978490650653839
Epoch 3, Batch 65, Batch Loss: 0.18940399587154388
Epoch 3, Batch 66, Batch Loss: 0.2168455421924591
Epoch 3, Batch 67, Batch Loss: 0.1583237498998642
Epoch 3, Batch 68, Batch Loss: 0.2033662647008896
Epoch 3, Batch 69, Batch Loss: 0.24919472634792328
Epoch 3, Batch 70, Batch Loss: 0.24027949571609497
Epoch 3, Batch 71, Batch Loss: 0.18907922506332397
Epoch 3, Batch 72, Batch Loss: 0.1906600147485733
Epoch 3, Batch 73, Batch Loss: 0.20917704701423645
Epoch 3, Batch 74, Batch Loss: 0.18875284492969513
Epoch 3, Batch 75, Batch Loss: 0.25001055002212524
Epoch 3, Batch 76, Batch Loss: 0.15918652713298798
Epoch 3, Average Loss: 0.21316584042812647
Epoch 4, Batch 1, Batch Loss: 0.13937808573246002
Epoch 4, Batch 2, Batch Loss: 0.16041141748428345
Epoch 4, Batch 3, Batch Loss: 0.1443382054567337
Epoch 4, Batch 4, Batch Loss: 0.19640357792377472
Epoch 4, Batch 5, Batch Loss: 0.14175903797149658
Epoch 4, Batch 6, Batch Loss: 0.1831067055463791
Epoch 4, Batch 7, Batch Loss: 0.14240576326847076
Epoch 4, Batch 8, Batch Loss: 0.152726411819458
Epoch 4, Batch 9, Batch Loss: 0.168229877948761
Epoch 4, Batch 10, Batch Loss: 0.16390787065029144
Epoch 4, Batch 11, Batch Loss: 0.13431908190250397
Epoch 4, Batch 12, Batch Loss: 0.1885063201189041
Epoch 4, Batch 13, Batch Loss: 0.11969361454248428
Epoch 4, Batch 14, Batch Loss: 0.17520859837532043
Epoch 4, Batch 15, Batch Loss: 0.16062213480472565
Epoch 4, Batch 16, Batch Loss: 0.14734357595443726
Epoch 4, Batch 17, Batch Loss: 0.16094116866588593
Epoch 4, Batch 18, Batch Loss: 0.20231011509895325
Epoch 4, Batch 19, Batch Loss: 0.11404218524694443
Epoch 4, Batch 20, Batch Loss: 0.20265591144561768
Epoch 4, Batch 21, Batch Loss: 0.12945692241191864
Epoch 4, Batch 22, Batch Loss: 0.15270620584487915
Epoch 4, Batch 23, Batch Loss: 0.21593351662158966
Epoch 4, Batch 24, Batch Loss: 0.19394215941429138
Epoch 4, Batch 25, Batch Loss: 0.18757027387619019
Epoch 4, Batch 26, Batch Loss: 0.1419539898633957
Epoch 4, Batch 27, Batch Loss: 0.14086954295635223
Epoch 4, Batch 28, Batch Loss: 0.14770473539829254
Epoch 4, Batch 29, Batch Loss: 0.17189989984035492
Epoch 4, Batch 30, Batch Loss: 0.24247632920742035
Epoch 4, Batch 31, Batch Loss: 0.1463623344898224
Epoch 4, Batch 32, Batch Loss: 0.14330683648586273
Epoch 4, Batch 33, Batch Loss: 0.11908277869224548
Epoch 4, Batch 34, Batch Loss: 0.13873650133609772
Epoch 4, Batch 35, Batch Loss: 0.138566255569458
Epoch 4, Batch 36, Batch Loss: 0.13265177607536316
Epoch 4, Batch 37, Batch Loss: 0.1464870572090149
Epoch 4, Batch 38, Batch Loss: 0.17647020518779755
Epoch 4, Batch 39, Batch Loss: 0.1594967246055603
Epoch 4, Batch 40, Batch Loss: 0.163545623421669
Epoch 4, Batch 41, Batch Loss: 0.15355679392814636
Epoch 4, Batch 42, Batch Loss: 0.10306274890899658
Epoch 4, Batch 43, Batch Loss: 0.16926707327365875
Epoch 4, Batch 44, Batch Loss: 0.2262016385793686
Epoch 4, Batch 45, Batch Loss: 0.18642869591712952
Epoch 4, Batch 46, Batch Loss: 0.15932045876979828
Epoch 4, Batch 47, Batch Loss: 0.18994447588920593
Epoch 4, Batch 48, Batch Loss: 0.16821728646755219
Epoch 4, Batch 49, Batch Loss: 0.15685150027275085
Epoch 4, Batch 50, Batch Loss: 0.14551123976707458
Epoch 4, Batch 51, Batch Loss: 0.18059583008289337
Epoch 4, Batch 52, Batch Loss: 0.16254638135433197
Epoch 4, Batch 53, Batch Loss: 0.14479438960552216
Epoch 4, Batch 54, Batch Loss: 0.1376500278711319
Epoch 4, Batch 55, Batch Loss: 0.19086693227291107
Epoch 4, Batch 56, Batch Loss: 0.17802980542182922
Epoch 4, Batch 57, Batch Loss: 0.19878126680850983
Epoch 4, Batch 58, Batch Loss: 0.22330154478549957
Epoch 4, Batch 59, Batch Loss: 0.17869508266448975
Epoch 4, Batch 60, Batch Loss: 0.11426225304603577
Epoch 4, Batch 61, Batch Loss: 0.14597204327583313
Epoch 4, Batch 62, Batch Loss: 0.22231462597846985
Epoch 4, Batch 63, Batch Loss: 0.1781567931175232
Epoch 4, Batch 64, Batch Loss: 0.14682278037071228
Epoch 4, Batch 65, Batch Loss: 0.15491187572479248
Epoch 4, Batch 66, Batch Loss: 0.2385406643152237
Epoch 4, Batch 67, Batch Loss: 0.1703193038702011
Epoch 4, Batch 68, Batch Loss: 0.1606350988149643
Epoch 4, Batch 69, Batch Loss: 0.1542290449142456
Epoch 4, Batch 70, Batch Loss: 0.14306055009365082
Epoch 4, Batch 71, Batch Loss: 0.170110821723938
Epoch 4, Batch 72, Batch Loss: 0.2048824429512024
Epoch 4, Batch 73, Batch Loss: 0.16647636890411377
Epoch 4, Batch 74, Batch Loss: 0.13095490634441376
Epoch 4, Batch 75, Batch Loss: 0.18848760426044464
Epoch 4, Batch 76, Batch Loss: 0.15988345444202423
Epoch 4, Average Loss: 0.16435754122702698
Epoch 5, Batch 1, Batch Loss: 0.13028524816036224
Epoch 5, Batch 2, Batch Loss: 0.1337021142244339
Epoch 5, Batch 3, Batch Loss: 0.14630736410617828
Epoch 5, Batch 4, Batch Loss: 0.10371842980384827
Epoch 5, Batch 5, Batch Loss: 0.10011670738458633
Epoch 5, Batch 6, Batch Loss: 0.10469093173742294
Epoch 5, Batch 7, Batch Loss: 0.16247780621051788
Epoch 5, Batch 8, Batch Loss: 0.13165603578090668
Epoch 5, Batch 9, Batch Loss: 0.1616128385066986
Epoch 5, Batch 10, Batch Loss: 0.1389971673488617
Epoch 5, Batch 11, Batch Loss: 0.12076979875564575
Epoch 5, Batch 12, Batch Loss: 0.12447138130664825
Epoch 5, Batch 13, Batch Loss: 0.15340474247932434
Epoch 5, Batch 14, Batch Loss: 0.12000307440757751
Epoch 5, Batch 15, Batch Loss: 0.15243655443191528
Epoch 5, Batch 16, Batch Loss: 0.13985763490200043
Epoch 5, Batch 17, Batch Loss: 0.157385915517807
Epoch 5, Batch 18, Batch Loss: 0.1400764286518097
Epoch 5, Batch 19, Batch Loss: 0.12861189246177673
Epoch 5, Batch 20, Batch Loss: 0.18776476383209229
Epoch 5, Batch 21, Batch Loss: 0.08801999688148499
Epoch 5, Batch 22, Batch Loss: 0.09007712453603745
Epoch 5, Batch 23, Batch Loss: 0.14006829261779785
Epoch 5, Batch 24, Batch Loss: 0.11007675528526306
Epoch 5, Batch 25, Batch Loss: 0.10766936093568802
Epoch 5, Batch 26, Batch Loss: 0.10348593443632126
Epoch 5, Batch 27, Batch Loss: 0.14114317297935486
Epoch 5, Batch 28, Batch Loss: 0.10528066754341125
Epoch 5, Batch 29, Batch Loss: 0.19875085353851318
Epoch 5, Batch 30, Batch Loss: 0.11790924519300461
Epoch 5, Batch 31, Batch Loss: 0.12997308373451233
Epoch 5, Batch 32, Batch Loss: 0.10116983205080032
Epoch 5, Batch 33, Batch Loss: 0.1500447690486908
Epoch 5, Batch 34, Batch Loss: 0.14156924188137054
Epoch 5, Batch 35, Batch Loss: 0.1273827701807022
Epoch 5, Batch 36, Batch Loss: 0.1407824158668518
Epoch 5, Batch 37, Batch Loss: 0.15320825576782227
Epoch 5, Batch 38, Batch Loss: 0.12618759274482727
Epoch 5, Batch 39, Batch Loss: 0.14170047640800476
Epoch 5, Batch 40, Batch Loss: 0.11428053677082062
Epoch 5, Batch 41, Batch Loss: 0.11553330719470978
Epoch 5, Batch 42, Batch Loss: 0.12920349836349487
Epoch 5, Batch 43, Batch Loss: 0.13041090965270996
Epoch 5, Batch 44, Batch Loss: 0.11735589802265167
Epoch 5, Batch 45, Batch Loss: 0.10164307057857513
Epoch 5, Batch 46, Batch Loss: 0.1130920797586441
Epoch 5, Batch 47, Batch Loss: 0.11310827732086182
Epoch 5, Batch 48, Batch Loss: 0.16578766703605652
Epoch 5, Batch 49, Batch Loss: 0.15144799649715424
Epoch 5, Batch 50, Batch Loss: 0.13355454802513123
Epoch 5, Batch 51, Batch Loss: 0.13374066352844238
Epoch 5, Batch 52, Batch Loss: 0.1528502106666565
Epoch 5, Batch 53, Batch Loss: 0.11573822796344757
Epoch 5, Batch 54, Batch Loss: 0.12957213819026947
Epoch 5, Batch 55, Batch Loss: 0.12323051691055298
Epoch 5, Batch 56, Batch Loss: 0.09553336352109909
Epoch 5, Batch 57, Batch Loss: 0.11996927112340927
Epoch 5, Batch 58, Batch Loss: 0.09532501548528671
Epoch 5, Batch 59, Batch Loss: 0.12003762274980545
Epoch 5, Batch 60, Batch Loss: 0.09312961250543594
Epoch 5, Batch 61, Batch Loss: 0.13032600283622742
Epoch 5, Batch 62, Batch Loss: 0.1127823069691658
Epoch 5, Batch 63, Batch Loss: 0.11904788762331009
Epoch 5, Batch 64, Batch Loss: 0.16861379146575928
Epoch 5, Batch 65, Batch Loss: 0.2002098709344864
Epoch 5, Batch 66, Batch Loss: 0.1314401924610138
Epoch 5, Batch 67, Batch Loss: 0.15521280467510223
Epoch 5, Batch 68, Batch Loss: 0.14688988029956818
Epoch 5, Batch 69, Batch Loss: 0.1567353755235672
Epoch 5, Batch 70, Batch Loss: 0.1444399505853653
Epoch 5, Batch 71, Batch Loss: 0.16400378942489624
Epoch 5, Batch 72, Batch Loss: 0.12911324203014374
Epoch 5, Batch 73, Batch Loss: 0.1265193223953247
Epoch 5, Batch 74, Batch Loss: 0.14328232407569885
Epoch 5, Batch 75, Batch Loss: 0.09867984056472778
Epoch 5, Batch 76, Batch Loss: 0.19530007243156433
Epoch 5, Average Loss: 0.13210510207634224
[2025-02-22 17:59:49,749] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Model and tokenizer saved to saved_model_trainsetC
